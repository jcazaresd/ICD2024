{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f719f59f-256e-4997-8022-8ebd910e434d",
   "metadata": {},
   "source": [
    "# Notebook ICD - 16\n",
    "### Librarias de confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17b7915-b165-4e45-98cf-7749c9d88f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9aa894-9381-47c5-a20e-8b3753932fa2",
   "metadata": {},
   "source": [
    "SVM from scratch\n",
    "This implementation follows the core principles of Support Vector Machines, where the main objective is to find the optimal hyperplane that separates classes. The classifier is initialized with three key parameters:\n",
    "\n",
    "* the learning rate controls how quickly the model adjusts during training,\n",
    "* the regularization parameter (lambda) prevents overfitting by balancing the margin and errors, and\n",
    "* the number of iterations sets how many times the algorithm should iterate over the dataset to optimize the hyperplane.\n",
    "\n",
    "Inside the class, the fit method is used to train the model by adjusting the weights (w) and bias (b) through gradient descent. During training, each data point is classified based on whether it satisfies the margin condition, and if it doesn't, both the weights and the bias are updated accordingly.\n",
    "\n",
    "The **predict** method uses the learned weights and bias to classify new instances by calculating the sign of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eea10c2-b744-444c-8d94-5aed673d0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=10):\n",
    "        \"\"\"\n",
    "        Initialize the SVM with hyperparameters for learning rate, regularization, and number of iterations.\n",
    "        \n",
    "        learning_rate: The step size for each iteration of gradient descent.\n",
    "        lambda_param: Regularization parameter to prevent overfitting.\n",
    "        n_iters: The number of times the algorithm will iterate over the dataset to find the optimal hyperplane.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate  # Controls the speed of convergence\n",
    "        self.lambda_param = lambda_param    # Regularization parameter (controls the margin)\n",
    "        self.n_iters = n_iters              # Number of training iterations\n",
    "        self.w = None                       # Weight vector (learned parameters)\n",
    "        self.b = None                       # Bias term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using the training data.\n",
    "        \n",
    "        X: The training feature matrix (n_samples, n_features)\n",
    "        y: The training labels (n_samples,). Labels should be in {-1, 1}.\n",
    "        \n",
    "        This method applies gradient descent to optimize the weights (w) and bias (b)\n",
    "        to maximize the margin between the classes.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)  # Convert labels to -1 and 1 for SVM\n",
    "\n",
    "        # Initialize the weight vector and bias term\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent optimization loop\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # If the condition holds, apply a regularization update (no penalty)\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    # If the condition fails, apply the update to w and b to penalize the misclassification\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.learning_rate * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for the test data.\n",
    "        \n",
    "        X: Test feature matrix.\n",
    "        \n",
    "        Returns the predicted labels (either -1 or 1).\n",
    "        \"\"\"\n",
    "        # Return the sign of the dot product (linear decision boundary)\n",
    "        return np.sign(np.dot(X, self.w) - self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7262b55-3bf4-43f1-82b3-edab1cb91599",
   "metadata": {},
   "source": [
    "### Implementation example\n",
    "Following the SVM definition, the Weather (Play Tennis) dataset is loaded into memory using pandas. This dataset includes several weather-related attributes such as outlook, temperature, humidity, and windy, which are used to predict whether tennis can be played. This stage prepares the raw data for further preprocessing.\n",
    "\n",
    "Next step, data preprocessing is essential because SVM requires numerical input. Each categorical feature in the dataset (e.g., sunny, hot, normal) is mapped to a corresponding integer value, allowing the algorithm to process the information effectively. The target label (whether tennis can be played or not) is also converted to numerical values (0 for \"no\", and 1 for \"yes\").\n",
    "\n",
    "After the preprocessing is complete, the dataset is split into features and labels. The features matrix (X) includes all the weather conditions (outlook, temperature, humidity, windy), while the labels vector (y) contains the target values (play tennis: yes or no). This separation allows the model to learn from the feature set while predicting the corresponding target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29e0a20c-0479-4c36-9c6f-f776e8309d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    outlook temperature humidity  windy play\n",
      "0     sunny         hot     high  False   no\n",
      "1     sunny         hot     high   True   no\n",
      "2  overcast         hot     high  False  yes\n",
      "3     rainy        mild     high  False  yes\n",
      "4     rainy        cool   normal  False  yes\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (assumed to be uploaded or present in the local system)\n",
    "data = pd.read_csv('weather.nominal - weather.nominal.csv')\n",
    "print(data.head())\n",
    "\n",
    "# Convert categorical variables into numerical values for KNN\n",
    "data['outlook'] = data['outlook'].map({'sunny': 0, 'overcast': 1, 'rainy': 2})\n",
    "data['temperature'] = data['temperature'].map({'hot': 0, 'mild': 1, 'cool': 2})\n",
    "data['humidity'] = data['humidity'].map({'high': 0, 'normal': 1})\n",
    "data['windy'] = data['windy'].astype(int)\n",
    "data['play'] = data['play'].map({'no': -1, 'yes': 1})\n",
    "\n",
    "# Define X (features) and y (labels)\n",
    "X = data.drop(columns='play').values  # Features\n",
    "y = data['play'].values  # Labels\n",
    "\n",
    "# Show the first transformed data\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3f449-460c-4449-ae67-ab6e5280a21e",
   "metadata": {},
   "source": [
    "Next, the SVM classifier is initialized with the specified hyperparameters, and the training process is executed using the fit method. The model iterates over the dataset multiple times, adjusting the hyperplane to maximize the margin between the two classes (yes and no). Through each iteration, it improves its prediction ability by refining the weight vector and bias to minimize classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d94f55c0-484c-4cb1-a774-badbcc0bb37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Train the SVM Classifier\n",
    "svm = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# This step initializes the SVM classifier with a specified learning rate, regularization parameter (lambda), and the number of iterations.\n",
    "# The `fit` method is used to train the classifier using the feature matrix X and the target labels y.\n",
    "# The model optimizes the weights and bias to create a hyperplane that separates the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a98ed5-8055-46a2-9030-e0ade159b2d8",
   "metadata": {},
   "source": [
    "Once the model is trained, a test instance is created to simulate new weather conditions (sunny, hot, normal humidity, and windy). This instance is represented as a numerical array, where each feature is encoded to match the format used during training. Finally, the classifier's predict method is employed to classify this test instance. Based on the learned hyperplane, the model outputs whether it is a suitable day to play tennis, and the result is displayed as either \"yes\" or \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3005d3db-27ee-4bb2-ac14-5c197daf18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the test instance: yes\n"
     ]
    }
   ],
   "source": [
    "# Create a Test Instance\n",
    "test_instance = np.array([[0, 0, 1, 1]])  # sunny, hot, normal, TRUE\n",
    "\n",
    "# This step defines a new test instance that represents a weather condition: sunny, hot, normal humidity, and windy.\n",
    "# The instance is passed as a NumPy array, where each feature is encoded numerically (sunny = 0, hot = 0, normal = 1, windy = TRUE = 1).\n",
    "\n",
    "# Make a Prediction\n",
    "prediction = svm.predict(test_instance)\n",
    "print(f\"Prediction for the test instance: {'yes' if prediction[0] == 1 else 'no'}\")\n",
    "\n",
    "# Finally, the trained SVM classifier makes a prediction on the test instance.\n",
    "# The `predict` method returns a label: either 1 (yes, play tennis) or -1 (no, don't play tennis).\n",
    "# The output is printed, interpreting the predicted label in the context of the weather conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d3e64-af65-4bb7-a8b2-a7b3d1234464",
   "metadata": {},
   "source": [
    "### Scikit-learn implementation\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are:\n",
    "\n",
    "Effective in high dimensional spaces.\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "SVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. C-Support Vector Classification (SVC) implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using LinearSVC or SGDClassifier instead, possibly after a Nystroem transformer or other Kernel Approximation.\n",
    "\n",
    "The multiclass support is handled according to a one-vs-one scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "780d02cd-fb38-43a3-b35d-1c47fa629643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "735b500d-0213-40e7-9e79-ee05fb3d7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'weather.numeric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ba370fb-7a63-440c-a426-46d5a07e6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook  Temperature  Humidity    Wind\n",
      "0     sunny           85        85    weak\n",
      "1     sunny           80        90  strong\n",
      "2  overcast           83        86    weak\n",
      "3      rain           70        96    weak\n",
      "4      rain           68        80    weak\n",
      "    Play\n",
      "0  False\n",
      "1  False\n",
      "2   True\n",
      "3   True\n",
      "4   True\n"
     ]
    }
   ],
   "source": [
    "# defining the dependent and independent variables\n",
    "X_train = df[['Outlook', 'Temperature', 'Humidity', 'Wind']]\n",
    "y_train = df[['Play']]\n",
    "\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6b64f37-6c5b-4cc3-8302-993ebcaed4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain']\n",
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n",
      "['weak', 'strong', 'weak', 'weak', 'weak', 'strong', 'strong', 'weak', 'weak', 'weak', 'strong', 'strong', 'weak', 'strong']\n",
      "[1 0 1 1 1 0 0 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "outlook = X_train.iloc[:,0]\n",
    "outlook_enc = encoder.fit_transform(outlook)\n",
    "print(outlook.tolist())\n",
    "print(outlook_enc)\n",
    "\n",
    "wind = X_train.iloc[:,3]\n",
    "wind_enc = encoder.fit_transform(wind)\n",
    "print(wind.tolist())\n",
    "print(wind_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "faf97a17-606d-40a7-ba2a-1388b2f372c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook  Temperature  Humidity  Wind\n",
      "0         2           85        85     2\n",
      "1         2           80        90     2\n",
      "2         0           83        86     0\n",
      "3         1           70        96     1\n",
      "4         1           68        80     1\n",
      "5         1           65        70     1\n",
      "6         0           64        65     0\n",
      "7         2           72        95     2\n",
      "8         2           69        70     2\n",
      "9         1           75        80     1\n",
      "10        2           75        70     2\n",
      "11        0           72        90     0\n",
      "12        0           81        75     0\n",
      "13        1           71        91     1\n"
     ]
    }
   ],
   "source": [
    "df_outlook = pd.DataFrame(outlook_enc, columns = ['Outlook'])\n",
    "df_wind = pd.DataFrame(outlook_enc, columns = ['Wind'])\n",
    "X_train_num = pd.concat([df_outlook, X_train.iloc[:,1], X_train.iloc[:,2], df_wind], axis=1)\n",
    "print(X_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6f3ba-74d2-419b-99da-ad5db1094984",
   "metadata": {},
   "source": [
    "#### Generación del modelo\n",
    "SVC, NuSVC and LinearSVC are classes capable of performing binary and multi-class classification on a dataset. SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations. On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5407918-208e-41f2-9721-af37a64a49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ingeo\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC().fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e93563-54e7-4aaf-9afc-c5f3b90a4171",
   "metadata": {},
   "source": [
    "### Evaluando modelo con nueva instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0248b90-486e-4d3a-be60-3c7ca2a8d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outlook  Temperature  Humidity  Wind\n",
      "0        2           60        65     1\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "# sunny:2, hot:85, normal:65, strong:0 \n",
    "new_example = [[2, 60, 65, 1]]\n",
    "X_test = pd.DataFrame(new_example, columns = ['Outlook', 'Temperature', 'Humidity', 'Wind'])\n",
    "print(X_test)\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc9aad-13f1-44b7-9cc6-410a25b194af",
   "metadata": {},
   "source": [
    "### Support vectors\n",
    "SVMs decision function (detailed in the Mathematical formulation) depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in attributes support_vectors_, support_ and n_support_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12269163-5e8e-4d86-93a4-d3343b7fdf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2. 85. 85.  2.]\n",
      " [ 2. 80. 90.  2.]\n",
      " [ 1. 65. 70.  1.]\n",
      " [ 2. 72. 95.  2.]\n",
      " [ 1. 71. 91.  1.]\n",
      " [ 0. 83. 86.  0.]\n",
      " [ 1. 70. 96.  1.]\n",
      " [ 1. 68. 80.  1.]\n",
      " [ 1. 75. 80.  1.]\n",
      " [ 2. 75. 70.  2.]\n",
      " [ 0. 72. 90.  0.]\n",
      " [ 0. 81. 75.  0.]]\n",
      "[ 0  1  5  7 13  2  3  4  9 10 11 12]\n",
      "[5 7]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print(clf.support_vectors_)\n",
    "\n",
    "# get indices of support vectors\n",
    "print(clf.support_)\n",
    "\n",
    "# get number of support vectors for each class\n",
    "print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd6401-8e84-4265-a15d-3710ff360430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
